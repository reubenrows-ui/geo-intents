{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2d344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "import google.auth\n",
    "from google.auth.transport.requests import Request\n",
    "from google.cloud import discoveryengine_v1 as discoveryengine\n",
    "from google.protobuf import json_format\n",
    "\n",
    "# --- Configuration ---\n",
    "# Replace with your project ID and desired location\n",
    "DATASTORE_PROJECT_ID = 'kaggle-hackathon-471317'\n",
    "DATASTORE_LOCATION = \"global\"  # e.g., \"us\", \"eu\", \"global\"\n",
    "DATASTORE_ID = \"poi-locations-ds-testrial12\"\n",
    "DATASTORE_DISPLAY_NAME = \"POI Locations Datastore\"\n",
    "DATASTORE_SCHEMA = {\n",
    "  \"type\": \"object\",\n",
    "  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "  \"dynamic\": \"false\",\n",
    "  \"datetime_detection\": True,\n",
    "  \"geolocation_detection\": True,\n",
    "  \"properties\": {\n",
    "    \"name\": {\n",
    "      \"indexable\": True,\n",
    "      \"searchable\": True,\n",
    "      \"type\": \"string\",\n",
    "      \"retrievable\": True,\n",
    "      \"dynamicFacetable\": True\n",
    "    },\n",
    "    \"category\": {\n",
    "      \"indexable\": True,\n",
    "      \"dynamicFacetable\": True,\n",
    "      \"searchable\": True,\n",
    "      \"retrievable\": True,\n",
    "      \"type\": \"string\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "    \n",
    "BQ_PROJECT_ID = 'kaggle-hackathon-471317' # Project containing the BQ table\n",
    "BQ_DATASET = \"geo_intents\"\n",
    "BQ_TABLE = \"sample_table\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17d538",
   "metadata": {},
   "source": [
    "### --- Core Functions ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a30eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_access_token() -> str:\n",
    "    \"\"\"Gets the default gcloud access token.\"\"\"\n",
    "    credentials, _ = google.auth.default(\n",
    "        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "    )\n",
    "    if not credentials.valid:\n",
    "        if credentials.expired and credentials.refresh_token:\n",
    "            credentials.refresh(Request())\n",
    "    return credentials.token\n",
    "\n",
    "\n",
    "def poll_operation_rest(operation_name: str, access_token: str) -> dict:\n",
    "    \"\"\"Polls a long-running operation using REST until it's done.\"\"\"\n",
    "    url = f\"https://discoveryengine.googleapis.com/v1/{operation_name}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        if result.get(\"done\", False):\n",
    "            print(\"Operation finished.\")\n",
    "            return result\n",
    "        print(\"Operation not finished, waiting 10 seconds...\")\n",
    "        time.sleep(10)\n",
    "\n",
    "\n",
    "# --- Core Functions ---\n",
    "\n",
    "def create_data_store(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    data_store_id: str,\n",
    "    display_name: str,\n",
    "    solution_type: str = \"SOLUTION_TYPE_SEARCH\",\n",
    ") -> discoveryengine.DataStore:\n",
    "    \"\"\"\n",
    "    Creates a new Vertex AI Search data store using the client library.\n",
    "\n",
    "    Args:\n",
    "        project_id: Your Google Cloud project ID.\n",
    "        location: The location for the data store (e.g., \"global\").\n",
    "        data_store_id: A unique ID for the new data store.\n",
    "        display_name: The display name for the data store.\n",
    "        solution_type: The solution type, e.g., 'SOLUTION_TYPE_SEARCH'.\n",
    "\n",
    "    Returns:\n",
    "        The created DataStore object.\n",
    "    \"\"\"\n",
    "    client = discoveryengine.DataStoreServiceClient()\n",
    "    parent = f\"projects/{project_id}/locations/{location}/collections/default_collection\"\n",
    "\n",
    "    data_store = discoveryengine.DataStore(\n",
    "        display_name=display_name,\n",
    "        solution_types=[solution_type],\n",
    "        content_config=discoveryengine.DataStore.ContentConfig.NO_CONTENT,\n",
    "        industry_vertical=discoveryengine.IndustryVertical.GENERIC,\n",
    "\n",
    "    )\n",
    "\n",
    "    request = discoveryengine.CreateDataStoreRequest(\n",
    "        parent=parent,\n",
    "        data_store=data_store,\n",
    "        data_store_id=data_store_id,\n",
    "    )\n",
    "\n",
    "    print(f\"Attempting to create data store '{data_store_id}'...\")\n",
    "    operation = client.create_data_store(request=request)\n",
    "\n",
    "    print(f\"Create data store operation started: {operation.operation.name}\")\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    response = operation.result()\n",
    "    print(f\"Data store '{response.name}' created successfully.\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def import_from_bigquery(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    data_store_id: str,\n",
    "    bigquery_project_id: str,\n",
    "    bigquery_dataset_id: str,\n",
    "    bigquery_table_id: str,\n",
    ") -> discoveryengine.ImportDocumentsResponse:\n",
    "    \"\"\"\n",
    "    Imports documents into a data store from a BigQuery table using the client library.\n",
    "\n",
    "    Args:\n",
    "        project_id: The project ID containing the data store.\n",
    "        location: The location of the data store.\n",
    "        data_store_id: The ID of the data store.\n",
    "        bigquery_project_id: The project ID of the BigQuery table.\n",
    "        bigquery_dataset_id: The dataset ID of the BigQuery table.\n",
    "        bigquery_table_id: The table ID of the BigQuery table.\n",
    "\n",
    "    Returns:\n",
    "        The ImportDocumentsResponse object.\n",
    "    \"\"\"\n",
    "    client = discoveryengine.DocumentServiceClient()\n",
    "\n",
    "    # The default branch is '0'\n",
    "    parent = client.branch_path(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        data_store=data_store_id,\n",
    "        branch=\"0\",\n",
    "    )\n",
    "\n",
    "    bq_source = discoveryengine.BigQuerySource(\n",
    "        project_id=bigquery_project_id,\n",
    "        dataset_id=bigquery_dataset_id,\n",
    "        table_id=bigquery_table_id,\n",
    "        data_schema=\"custom\",\n",
    "    )\n",
    "\n",
    "    request = discoveryengine.ImportDocumentsRequest(\n",
    "        parent=parent,\n",
    "        bigquery_source=bq_source\n",
    "        # The options below are optional\n",
    "        # reconciliation_mode defaults to INCREMENTAL\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Starting import from BigQuery table '{bigquery_project_id}.{bigquery_dataset_id}.{bigquery_table_id}'...\"\n",
    "    )\n",
    "    operation = client.import_documents(request=request)\n",
    "\n",
    "    print(f\"Import operation started: {operation.operation.name}\")\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    response = operation.result()\n",
    "    print(\"Import completed successfully.\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def update_schema(\n",
    "    project_id: str, location: str, data_store_id: str, schema_dict: dict\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Updates the schema of a data store using the REST API.\n",
    "\n",
    "    Args:\n",
    "        project_id: The project ID containing the data store.\n",
    "        location: The location of the data store.\n",
    "        data_store_id: The ID of the data store.\n",
    "        schema_dict: A dictionary representing the JSON schema.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the response of the completed operation.\n",
    "    \"\"\"\n",
    "    access_token = get_access_token()\n",
    " \n",
    "    # For PATCH requests, the fields to be updated must be specified in an `updateMask` query parameter.\n",
    "    # The field for a JSON object schema is `structSchema`.\n",
    "    url = f\"https://discoveryengine.googleapis.com/v1/projects/{project_id}/locations/{location}/collections/default_collection/dataStores/{data_store_id}/schemas/default_schema\" \\\n",
    "\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    # The payload is the Schema resource itself, with only the fields to be updated.\n",
    "    # The key for a JSON object schema is `structSchema`.\n",
    "    payload = {\"structSchema\": schema_dict}\n",
    "\n",
    "    print(f\"Attempting to update schema for data store '{data_store_id}' via REST...\")\n",
    "    response = requests.patch(url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error updating schema: {response.text}\")\n",
    "        response.raise_for_status()\n",
    "\n",
    "    operation = response.json()\n",
    "    print(f\"Update schema operation started: {operation['name']}\")\n",
    "\n",
    "    # Poll the operation for completion\n",
    "    final_result = poll_operation_rest(operation[\"name\"], access_token)\n",
    "\n",
    "    print(\"Schema updated successfully.\")\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def get_schema(project_id: str, location: str, data_store_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves the current schema of a data store using the client library.\n",
    "\n",
    "    Args:\n",
    "        project_id: The project ID containing the data store.\n",
    "        location: The location of the data store.\n",
    "        data_store_id: The ID of the data store.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the current schema.\n",
    "    \"\"\"\n",
    "    client = discoveryengine.SchemaServiceClient()\n",
    "\n",
    "    # The default schema is named 'default_schema'\n",
    "    schema_name = client.schema_path(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        data_store=data_store_id,\n",
    "        schema=\"default_schema\",\n",
    "    )\n",
    "\n",
    "    request = discoveryengine.GetSchemaRequest(name=schema_name)\n",
    "\n",
    "    print(f\"Attempting to retrieve schema for data store '{data_store_id}'...\")\n",
    "    schema = client.get_schema(request=request)\n",
    "\n",
    "    # Convert the protobuf Struct back to a Python dictionary for easy viewing\n",
    "    # The schema can be in either structSchema or jsonSchema field\n",
    "    if schema.struct_schema:\n",
    "        return json_format.MessageToDict(schema.struct_schema)\n",
    "    elif schema.json_schema:\n",
    "        return json.loads(schema.json_schema)\n",
    "    return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b420c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create data store 'poi-locations-ds-testrial12'...\n",
      "Create data store operation started: projects/168027251845/locations/global/collections/default_collection/operations/create-data-store-12936732990538311071\n",
      "Waiting for operation to complete...\n",
      "Data store 'projects/168027251845/locations/global/collections/default_collection/dataStores/poi-locations-ds-testrial12' created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_result = create_data_store(\n",
    "    project_id=DATASTORE_PROJECT_ID,\n",
    "    location=DATASTORE_LOCATION,\n",
    "    data_store_id=DATASTORE_ID,\n",
    "    display_name=DATASTORE_DISPLAY_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d2114b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting import from BigQuery table 'kaggle-hackathon-471317.geo_intents.sample_table'...\n",
      "Import operation started: projects/168027251845/locations/global/collections/default_collection/dataStores/poi-locations-ds-testrial12/branches/0/operations/import-documents-7910644913709426476\n",
      "Waiting for operation to complete...\n",
      "Import completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import_result = import_from_bigquery(\n",
    "    project_id=DATASTORE_PROJECT_ID,\n",
    "    location=DATASTORE_LOCATION,\n",
    "    data_store_id=DATASTORE_ID,\n",
    "    bigquery_project_id=BQ_PROJECT_ID,\n",
    "    bigquery_dataset_id=BQ_DATASET,\n",
    "    bigquery_table_id=BQ_TABLE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d41d2887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to update schema for data store 'poi-locations-ds-testrial12' via REST...\n",
      "Update schema operation started: projects/168027251845/locations/global/collections/default_collection/dataStores/poi-locations-ds-testrial12/schemas/default_schema/operations/update-schema-17342063653246973095\n",
      "Operation not finished, waiting 10 seconds...\n",
      "Operation finished.\n",
      "Schema updated successfully.\n"
     ]
    }
   ],
   "source": [
    "update_schema_result = update_schema(\n",
    "    project_id=DATASTORE_PROJECT_ID,\n",
    "    location=DATASTORE_LOCATION,\n",
    "    data_store_id=DATASTORE_ID,\n",
    "    schema_dict=DATASTORE_SCHEMA,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
